{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdcution\n",
    "\n",
    "Have you ever wondered how much a captivating picture can influence your decision to adopt or facilitate the adoption of an animal? In the world of the animal welfare, where countless of stray animals are suffering on the streets or in shelters every day, the power of a single image extends beyond aesthetics – it serves as a potential lifeline. With the rise of data science, we aim to help by predicting the appealingness of an image through the \"Pawpularity\" project to hopefully increase the chances of finding caring homes for these rescue animals.\n",
    "\n",
    "PetFinder.my is Malaysia’s leading animal welfare platform and it collaborates closely with animal lovers, media, corporations, and global organizations to improve animal welfare. Currently it analyzes cuteness and other factors to rank animals images, which is still in an experimental stage with rooms for improvements. By analyzing both raw images and the metadata, we seek to enhance PetFinder.my's existing approach and provide actionable insights to increase adoption rates. Through this endeavor, we strive to not only improve the fortunes of individual animals but also contribute to the broader cause of animal welfare worldwide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns  # for nicer plots\n",
    "sns.set(style=\"darkgrid\")  # default style\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) #used to supress the tf version warning. \n",
    "\n",
    "# FILL IN CODE HERE #\n",
    "IMAGE_PATH = \"./Working/train/\"\n",
    "MEGA_FILE = \"./Working/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import images & labels\n",
    "def load_data(path_to_images, path_to_label):\n",
    "    '''Load 2D images and their corresponding labels\n",
    "    Parameters:\n",
    "    path_to_data (str): This is the path to data\n",
    "    \n",
    "    Returns:\n",
    "    images (np.ndarray): A numpy array of shape (N, 64, 64, 3)\n",
    "    labels (np.ndarray): A numpy array of shape (N)\n",
    "    '''\n",
    "    #Initialize images and labels as empty list\n",
    "    images, labels = [], []\n",
    "\n",
    "\n",
    "    for item in os.listdir(path_to_images):\n",
    "\n",
    "        #Load image and convert image to array\n",
    "        #Resize images to 64x64x3\n",
    "        image = img_to_array(load_img(os.path.join(path_to_images, item)))\n",
    "        resized_image = tf.image.resize(image, [128, 128])\n",
    "\n",
    "        #append image and label to the final respective list\n",
    "        images.append(resized_image)\n",
    "\n",
    "    mega_file = pd.read_csv(path_to_label)\n",
    "    labels = mega_file[\"Pawpularity\"]\n",
    "\n",
    "    #Convert list to numpy array\n",
    "    images, labels = np.array(images), np.array(labels)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of images: (9912, 128, 128, 3)\n",
      "Shape of labels: (9912,)\n"
     ]
    }
   ],
   "source": [
    "#Load the images and labels\n",
    "images, labels = load_data(IMAGE_PATH, MEGA_FILE)\n",
    "print(\"Shape of images:\", images.shape)\n",
    "print(\"Shape of labels:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train images shape: (7929, 128, 128, 3)\n",
      "train labels shape: (7929,)\n",
      "test images shape: (1983, 128, 128, 3)\n",
      "test labels shape: (1983,)\n"
     ]
    }
   ],
   "source": [
    "## Use a 80/20 train/test split.\n",
    "images_train, images_test, labels_train, labels_test = train_test_split(images, labels, test_size=0.2, random_state=100)    \n",
    "\n",
    "images_train = images_train / 255\n",
    "images_test = images_test / 255\n",
    "\n",
    "print('train images shape:', images_train.shape)\n",
    "print('train labels shape:', labels_train.shape)\n",
    "print('test images shape:', images_test.shape)\n",
    "print('test labels shape:', labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply random shufflying to training examples.\n",
    "np.random.seed(100)\n",
    "indices = np.arange(images_train.shape[0])\n",
    "shuffled_indices = np.random.permutation(indices)\n",
    "images_train = images_train[shuffled_indices]\n",
    "labels_train = labels_train[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for train data baseline output: \n",
      " [37.95913734 37.95913734 37.95913734 ... 37.95913734 37.95913734\n",
      " 37.95913734] \n",
      "\n",
      "Predictions for testing set baseline output: \n",
      " [37.95913734 37.95913734 37.95913734 ... 37.95913734 37.95913734\n",
      " 37.95913734] \n",
      "\n",
      "RMSE of the baseline on the train data: \n",
      " 20.599719353543417 \n",
      "\n",
      "RMSE of the baseline on the test data: \n",
      " 20.55663146623801\n"
     ]
    }
   ],
   "source": [
    "#Set up baseline model\n",
    "def predict_baseline(X):\n",
    "    \n",
    "    # Calculate the average score for all labels\n",
    "    avg_score = labels_train.mean()\n",
    "    \n",
    "    # Get number of examples in the training set\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Generate predictions\n",
    "    Y = np.dot(np.ones(m), avg_score)\n",
    "    \n",
    "    return Y\n",
    "\n",
    "def RMSE(X, Y):\n",
    "    \n",
    "    # Generate predictions of baseline on the train data\n",
    "    Y_predict = predict_baseline(X)\n",
    "\n",
    "    # Calculate the difference between predictions and actual prices\n",
    "    diff = Y_predict - Y\n",
    "\n",
    "    # Compute the RMSE\n",
    "    m = X.shape[0]\n",
    "    RMSE =  np.sqrt(np.sum(np.square(diff)) / m)\n",
    "    \n",
    "    return RMSE\n",
    "\n",
    "\n",
    "print(\"Predictions for train data baseline output: \\n\", predict_baseline(images_train), \"\\n\")\n",
    "print(\"Predictions for testing set baseline output: \\n\", predict_baseline(images_test), \"\\n\")\n",
    "\n",
    "RMSE_baseline_train = RMSE(images_train, labels_train)\n",
    "RMSE_baseline_test = RMSE(images_test, labels_test)\n",
    "print(\"RMSE of the baseline on the train data: \\n\", RMSE_baseline_train, \"\\n\")\n",
    "print(\"RMSE of the baseline on the test data: \\n\", RMSE_baseline_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 16s 256ms/step - loss: 422.8158 - accuracy: 0.0000e+00\n",
      "Final RMSE on test data is 20.562484416549946\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN model\n",
    "def build_cnn():\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(\n",
    "        filters=64, kernel_size=5, activation=\"relu\", padding=\"same\"))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(tf.keras.layers.Conv2D(\n",
    "        filters=128, kernel_size=5, activation=\"relu\", padding=\"same\"))   \n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(tf.keras.layers.Conv2D(\n",
    "        filters=256, kernel_size=5, activation=\"relu\", padding=\"same\"))   \n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=512, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
    "                  loss=\"mean_squared_error\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "CNN = build_cnn()\n",
    "\n",
    "# use early stopping\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(patience=3)\n",
    "\n",
    "# Train the model\n",
    "history = CNN.fit(images_train,\n",
    "                  labels_train,\n",
    "                  epochs=20,\n",
    "                  validation_split=0.1,\n",
    "                  verbose=0,\n",
    "                  callbacks=[early_stopping_callback])\n",
    "\n",
    "train_loss, train_metric = CNN.evaluate(images_train, labels_train)\n",
    "train_RMSE = np.sqrt(train_loss)\n",
    "print(\"Final RMSE on train data is\", train_RMSE)\n",
    "\n",
    "test_loss, test_metric = CNN.evaluate(images_test, labels_test)\n",
    "test_RMSE = np.sqrt(test_loss)\n",
    "print(\"Final RMSE on test data is\", test_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 64s 257ms/step - loss: 424.9432 - accuracy: 5.0448e-04\n",
      "Final RMSE on train data is 20.61415138453891\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
